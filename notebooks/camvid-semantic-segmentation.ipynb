{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data Exploaraion"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:34:05.449070Z","iopub.status.busy":"2023-11-15T06:34:05.448742Z","iopub.status.idle":"2023-11-15T06:34:05.455212Z","shell.execute_reply":"2023-11-15T06:34:05.454530Z","shell.execute_reply.started":"2023-11-15T06:34:05.449037Z"},"id":"uiPEYNh19Nqw","trusted":true},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:34:38.954169Z","iopub.status.busy":"2023-11-15T06:34:38.953794Z","iopub.status.idle":"2023-11-15T06:34:38.958299Z","shell.execute_reply":"2023-11-15T06:34:38.957184Z","shell.execute_reply.started":"2023-11-15T06:34:38.954134Z"},"id":"gIm4Uo4IJRux","trusted":true},"outputs":[],"source":["data_path = \"/kaggle/input/camvid/CamVid\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:34:40.461999Z","iopub.status.busy":"2023-11-15T06:34:40.461537Z","iopub.status.idle":"2023-11-15T06:34:41.041044Z","shell.execute_reply":"2023-11-15T06:34:41.040190Z","shell.execute_reply.started":"2023-11-15T06:34:40.461952Z"},"id":"hj6dRP8c9zFI","outputId":"719a0993-7226-41d0-ad25-bcdd13487660","trusted":true},"outputs":[],"source":["print('Number of train frames: ' + str(len(os.listdir(data_path + '/'+ 'train'))))\n","print('Number of train labels: ' + str(len(os.listdir(data_path + '/'+ 'train_labels'))))\n","print('Number of val frames: ' + str(len(os.listdir(data_path + '/'+ 'val'))))\n","print('Number of val labels: ' + str(len(os.listdir(data_path + '/'+ 'val_labels'))))\n","print('Number of test frames: ' + str(len(os.listdir(data_path + '/'+ 'test'))))\n","print('Number of test labels: ' + str(len(os.listdir(data_path + '/'+ 'test_labels'))))\n","print('Total frames: ' + str(len(os.listdir(data_path + '/'+ 'train')) + len(os.listdir(data_path + '/'+ 'val')) + len(os.listdir(data_path + '/'+ 'test'))))"]},{"cell_type":"markdown","metadata":{"id":"7C6OliBmA0rP"},"source":["Now, let's see which classes we have. This can be found in the original CAMVID [text file](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/data/label_colors.txt). However, under the same repo, the author has dumped it into csv which we will use."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:34:50.404585Z","iopub.status.busy":"2023-11-15T06:34:50.404226Z","iopub.status.idle":"2023-11-15T06:34:50.426888Z","shell.execute_reply":"2023-11-15T06:34:50.425975Z","shell.execute_reply.started":"2023-11-15T06:34:50.404530Z"},"id":"FKdmFX1DKDud","trusted":true},"outputs":[],"source":["import pandas as pd\n","classes = pd.read_csv(data_path+'/class_dict.csv', index_col =0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:34:50.838520Z","iopub.status.busy":"2023-11-15T06:34:50.838191Z","iopub.status.idle":"2023-11-15T06:34:50.862286Z","shell.execute_reply":"2023-11-15T06:34:50.861066Z","shell.execute_reply.started":"2023-11-15T06:34:50.838489Z"},"trusted":true},"outputs":[],"source":["classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:34:51.398464Z","iopub.status.busy":"2023-11-15T06:34:51.398137Z","iopub.status.idle":"2023-11-15T06:34:51.403853Z","shell.execute_reply":"2023-11-15T06:34:51.402974Z","shell.execute_reply.started":"2023-11-15T06:34:51.398435Z"},"id":"yH17IqAxGdN4","outputId":"863aa3a0-20df-43dd-ed94-8e08b61e7869","trusted":true},"outputs":[],"source":["n_classes = len(classes)\n","n_classes"]},{"cell_type":"markdown","metadata":{"id":"WYOiawzIBHWC"},"source":["**This data frame maps the class names to colors.**\n","\n","**To access the colors, we can index the dataframe with its row index name using the .loc operation.**\n"]},{"cell_type":"markdown","metadata":{"id":"rMWbSrv2B3Ns"},"source":["Now we are ready to create a map from class name to color"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:34:53.626242Z","iopub.status.busy":"2023-11-15T06:34:53.625901Z","iopub.status.idle":"2023-11-15T06:34:53.636351Z","shell.execute_reply":"2023-11-15T06:34:53.635329Z","shell.execute_reply.started":"2023-11-15T06:34:53.626213Z"},"id":"u9QZkCt87fAj","trusted":true},"outputs":[],"source":["cls2rgb = {cl:list(classes.loc[cl, :]) for cl in classes.index}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:34:54.736326Z","iopub.status.busy":"2023-11-15T06:34:54.735961Z","iopub.status.idle":"2023-11-15T06:34:54.744674Z","shell.execute_reply":"2023-11-15T06:34:54.743745Z","shell.execute_reply.started":"2023-11-15T06:34:54.736289Z"},"trusted":true},"outputs":[],"source":["cls2rgb"]},{"cell_type":"markdown","metadata":{"id":"8BAQSaWvGirG"},"source":["## Now let's visualize and explore some samples:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:34:56.417585Z","iopub.status.busy":"2023-11-15T06:34:56.417192Z","iopub.status.idle":"2023-11-15T06:34:56.430088Z","shell.execute_reply":"2023-11-15T06:34:56.429169Z","shell.execute_reply.started":"2023-11-15T06:34:56.417528Z"},"trusted":true},"outputs":[],"source":["from glob import glob\n","import numpy as np\n","train_img_loc = sorted(np.array(glob(\"/kaggle/input/camvid/CamVid/train/*\")))\n","train_labels_loc = sorted(np.array(glob(\"/kaggle/input/camvid/CamVid/train_labels/*\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:36:17.496607Z","iopub.status.busy":"2023-11-15T06:36:17.496232Z","iopub.status.idle":"2023-11-15T06:36:18.074512Z","shell.execute_reply":"2023-11-15T06:36:18.073607Z","shell.execute_reply.started":"2023-11-15T06:36:17.496543Z"},"id":"Q7f1_yHcm3Uh","outputId":"58b5364a-6172-4e9f-b0f5-7fb68d535a8b","trusted":true},"outputs":[],"source":["%matplotlib inline\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","\n","idx = 0\n","img = cv2.imread(train_img_loc[idx])\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","plt.imshow(img)\n","print(img.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T04:14:40.324188Z","iopub.status.busy":"2023-11-01T04:14:40.323867Z","iopub.status.idle":"2023-11-01T04:14:41.207275Z","shell.execute_reply":"2023-11-01T04:14:41.206471Z","shell.execute_reply.started":"2023-11-01T04:14:40.324159Z"},"trusted":true},"outputs":[],"source":["import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T10:44:51.746242Z","iopub.status.busy":"2023-10-25T10:44:51.745768Z","iopub.status.idle":"2023-10-25T10:44:57.018588Z","shell.execute_reply":"2023-10-25T10:44:57.016565Z","shell.execute_reply.started":"2023-10-25T10:44:51.746204Z"},"trusted":true},"outputs":[],"source":["img_arr = img.reshape(-1, 3) # combining the width and height and keeping the channels\n","kmeans = KMeans(n_clusters=5)\n","kmeans.fit(img_arr)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T10:44:57.023320Z","iopub.status.busy":"2023-10-25T10:44:57.021817Z","iopub.status.idle":"2023-10-25T10:44:57.047006Z","shell.execute_reply":"2023-10-25T10:44:57.045569Z","shell.execute_reply.started":"2023-10-25T10:44:57.023246Z"},"trusted":true},"outputs":[],"source":["segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n","segmented_img = segmented_img.reshape(img.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T10:44:57.051577Z","iopub.status.busy":"2023-10-25T10:44:57.050958Z","iopub.status.idle":"2023-10-25T10:44:57.441855Z","shell.execute_reply":"2023-10-25T10:44:57.440061Z","shell.execute_reply.started":"2023-10-25T10:44:57.051510Z"},"trusted":true},"outputs":[],"source":["plt.imshow(segmented_img / 255)"]},{"cell_type":"markdown","metadata":{"id":"kpJ4GpiuGvvB"},"source":["Let's have a look on the masks (the ground truth)"]},{"cell_type":"markdown","metadata":{"id":"yQPgxveDG1P7"},"source":["As you can see the masks are just colors (L,W,3).\n","What we actually want is a (L,W) matrix, with each value is from 0 to 31 representing the 32 class labels."]},{"cell_type":"markdown","metadata":{"id":"nMBOk3-3QuLr"},"source":["Colors are different from the colors in cls2rgb! Because the order is BGR not RGB when using cv2.imread: https://stackoverflow.com/questions/46898979/how-to-check-the-channel-order-of-an-image\n","\n","If you want to get the same order as in the color mapping of CAMVID, use the cv converted"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:36:23.248041Z","iopub.status.busy":"2023-11-15T06:36:23.247711Z","iopub.status.idle":"2023-11-15T06:36:23.273673Z","shell.execute_reply":"2023-11-15T06:36:23.272639Z","shell.execute_reply.started":"2023-11-15T06:36:23.248012Z"},"id":"7dalUjddkDEb","trusted":true},"outputs":[],"source":["import numpy as np\n","\n","idx = 0\n","mask = cv2.imread(train_labels_loc[idx])\n","mask = cv2.cvtColor((mask).astype(np.uint8), cv2.COLOR_BGR2RGB)# If you want to get the same order as in the color mapping of CAMVID, use the cv converted"]},{"cell_type":"markdown","metadata":{"id":"C533H1FPQ_Uh"},"source":["Now if you plot the mask again, you will see different colors. For example the red and blue are reversed than before:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:36:28.806945Z","iopub.status.busy":"2023-11-15T06:36:28.806593Z","iopub.status.idle":"2023-11-15T06:36:29.028655Z","shell.execute_reply":"2023-11-15T06:36:29.027846Z","shell.execute_reply.started":"2023-11-15T06:36:28.806914Z"},"id":"mU_LOU2tkF0F","outputId":"6378aabf-ab07-4ef0-f960-e799562f9fd9","trusted":true},"outputs":[],"source":["plt.imshow(mask)\n","print(mask.shape)"]},{"cell_type":"markdown","metadata":{"id":"FcY60Jg4RSl7"},"source":["Another solution is to use load_image from keras which uses RGB (it uses PIL under the hood) unlike cv2.imread"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:36:31.327461Z","iopub.status.busy":"2023-11-15T06:36:31.327128Z","iopub.status.idle":"2023-11-15T06:36:36.500452Z","shell.execute_reply":"2023-11-15T06:36:36.499642Z","shell.execute_reply.started":"2023-11-15T06:36:31.327433Z"},"id":"FPnvFddHi0z7","outputId":"6317fd9f-5b1e-4e59-cda3-30a45127198b","trusted":true},"outputs":[],"source":["from keras.preprocessing.image import load_img\n","# mask = load_img(str(data_path) + '/train_labels/0001TP_006690_L.png')\n","mask = load_img(train_labels_loc[0])\n","mask\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T10:45:12.813147Z","iopub.status.busy":"2023-10-25T10:45:12.812783Z","iopub.status.idle":"2023-10-25T10:45:12.820501Z","shell.execute_reply":"2023-10-25T10:45:12.818921Z","shell.execute_reply.started":"2023-10-25T10:45:12.813112Z"},"id":"7QYCJFNBjB0-","trusted":true},"outputs":[],"source":["mask = np.array(mask)# Now colors are the same as in the dict, since keras load_img uses RGB order."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T10:45:12.823028Z","iopub.status.busy":"2023-10-25T10:45:12.822483Z","iopub.status.idle":"2023-10-25T10:45:12.839481Z","shell.execute_reply":"2023-10-25T10:45:12.837500Z","shell.execute_reply.started":"2023-10-25T10:45:12.822975Z"},"id":"ZEkNOms5rOFc","outputId":"31917568-53dd-4639-81e7-136bca0f55d3","trusted":true},"outputs":[],"source":["mask.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T10:45:12.842131Z","iopub.status.busy":"2023-10-25T10:45:12.841616Z","iopub.status.idle":"2023-10-25T10:45:12.895549Z","shell.execute_reply":"2023-10-25T10:45:12.894564Z","shell.execute_reply.started":"2023-10-25T10:45:12.842078Z"},"trusted":true},"outputs":[],"source":["np.unique(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T10:45:12.898361Z","iopub.status.busy":"2023-10-25T10:45:12.897982Z","iopub.status.idle":"2023-10-25T10:45:12.906035Z","shell.execute_reply":"2023-10-25T10:45:12.904972Z","shell.execute_reply.started":"2023-10-25T10:45:12.898323Z"},"id":"DD7yMe_Oc0qy","outputId":"8959f678-0de7-433b-cbfe-08c8703142e0","trusted":true},"outputs":[],"source":["mask.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-25T10:45:13.174332Z","iopub.status.busy":"2023-10-25T10:45:13.173923Z","iopub.status.idle":"2023-10-25T10:45:13.457703Z","shell.execute_reply":"2023-10-25T10:45:13.456644Z","shell.execute_reply.started":"2023-10-25T10:45:13.174296Z"},"trusted":true},"outputs":[],"source":["plt.imshow(np.where([[64, 64, 0]], mask, 0))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:36:37.901254Z","iopub.status.busy":"2023-11-15T06:36:37.900913Z","iopub.status.idle":"2023-11-15T06:36:37.916625Z","shell.execute_reply":"2023-11-15T06:36:37.915539Z","shell.execute_reply.started":"2023-11-15T06:36:37.901225Z"},"trusted":true},"outputs":[],"source":["idx = 0\n","mask = cv2.imread(train_labels_loc[idx])\n","mask = cv2.cvtColor((mask).astype(np.uint8), cv2.COLOR_BGR2RGB)# If you want to get the same order as in the color mapping of CAMVID, use the cv converted"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:36.692114Z","iopub.status.busy":"2023-11-01T09:03:36.691816Z","iopub.status.idle":"2023-11-01T09:03:37.559713Z","shell.execute_reply":"2023-11-01T09:03:37.558787Z","shell.execute_reply.started":"2023-11-01T09:03:36.692086Z"},"trusted":true},"outputs":[],"source":["class_clr = [ color for color in list( cls2rgb.values() ) ]\n","\n","class_name = list( cls2rgb.keys() )\n","\n","# class_id = 1\n","for class_id in range(32):\n","    true_case = np.float32( np.all( np.equal( class_clr[ class_id ], mask ), axis=-1 ) * 1 )\n","\n","    if true_case.any():\n","        contours, _ = cv2.findContours(true_case.astype('uint8'), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","        for contour in contours:\n","            # Get the bounding rectangle of the contour\n","            x, y, w, h = cv2.boundingRect(contour)\n","\n","            # Draw the rectangle on the mask\n","            cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n","            cv2.putText(img, class_name[class_id], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","    else:\n","        continue\n","\n","plt.imshow(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T04:16:39.612681Z","iopub.status.busy":"2023-11-01T04:16:39.612317Z","iopub.status.idle":"2023-11-01T04:16:39.667118Z","shell.execute_reply":"2023-11-01T04:16:39.666185Z","shell.execute_reply.started":"2023-11-01T04:16:39.612643Z"},"trusted":true},"outputs":[],"source":["idx = 0\n","img = cv2.imread(train_img_loc[idx])\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","# plt.imshow(img)\n","# print(img.shape)\n","\n","pedestrian = 26\n","true_case = np.float32( np.all( np.equal( class_clr[ pedestrian ], mask ), axis=-1 ) * 1 )\n","\n","if true_case.any():\n","    contours, _ = cv2.findContours(true_case.astype('uint8'), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    for contour in contours:\n","        # Get the bounding rectangle of the contour\n","        x, y, w, h = cv2.boundingRect(contour)\n","\n","        # Draw the rectangle on the mask\n","        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n","        cv2.putText(img, class_name[pedestrian], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T04:16:44.630820Z","iopub.status.busy":"2023-11-01T04:16:44.630457Z","iopub.status.idle":"2023-11-01T04:16:45.124493Z","shell.execute_reply":"2023-11-01T04:16:45.123641Z","shell.execute_reply.started":"2023-11-01T04:16:44.630784Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(30, 10))\n","\n","plt.imshow(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T04:17:35.161880Z","iopub.status.busy":"2023-11-01T04:17:35.161498Z","iopub.status.idle":"2023-11-01T04:17:35.750383Z","shell.execute_reply":"2023-11-01T04:17:35.749338Z","shell.execute_reply.started":"2023-11-01T04:17:35.161849Z"},"trusted":true},"outputs":[],"source":["c_st =  []\n","for colour in list( cls2rgb.values() ):   \n","    c_st.append( colour )\n","    \n","class_name = list( (cls2rgb.keys()) ) \n","class_id = 26\n","\n","case_all_class = np.float32( np.equal( c_st[ class_id ], mask ) * 1)\n","case_true_class =  np.float32(np.all(np.equal( c_st[ class_id ], mask), axis = -1 ) *1 )\n","\n","\n","figsize=(15, 3)\n","_, axes = plt.subplots( nrows=1, ncols= 3, figsize=figsize )\n","\n","axes[0].imshow( mask )\n","axes[1].imshow( case_all_class )\n","axes[2].imshow( case_true_class, cmap='gray' )\n","\n","print('name of the class:', class_name[ class_id ] )\n","\n","# plt.imshow(case_all_class)\n","# plt.imshow(case_true_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:36:48.122623Z","iopub.status.busy":"2023-11-15T06:36:48.122269Z","iopub.status.idle":"2023-11-15T06:36:48.129972Z","shell.execute_reply":"2023-11-15T06:36:48.128949Z","shell.execute_reply.started":"2023-11-15T06:36:48.122593Z"},"id":"2h36dbNQ8Jvj","trusted":true},"outputs":[],"source":["def adjust_mask(mask, flat=False):\n","    \n","    semantic_map = []\n","    for colour in list(cls2rgb.values()):        \n","        equality = np.equal(mask, colour)# 256x256x3 with True or False\n","        class_map = np.all(equality, axis = -1)# 256x256 If all True, then True, else False\n","        semantic_map.append(class_map)# List of 256x256 arrays, map of True for a given found color at the pixel, and False otherwise.\n","    semantic_map = np.stack(semantic_map, axis=-1)# 256x256x32 True only at the found color, and all False otherwise.\n","    if flat:\n","        semantic_map = np.reshape(semantic_map, (-1,256*256))\n","\n","    return np.float32(semantic_map)# convert to numbers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:36:51.949704Z","iopub.status.busy":"2023-11-15T06:36:51.949368Z","iopub.status.idle":"2023-11-15T06:36:52.566737Z","shell.execute_reply":"2023-11-15T06:36:52.565895Z","shell.execute_reply.started":"2023-11-15T06:36:51.949676Z"},"id":"f-msSAjN8Li6","trusted":true},"outputs":[],"source":["new_mask = adjust_mask(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:36:53.924746Z","iopub.status.busy":"2023-11-15T06:36:53.924400Z","iopub.status.idle":"2023-11-15T06:36:53.930284Z","shell.execute_reply":"2023-11-15T06:36:53.929267Z","shell.execute_reply.started":"2023-11-15T06:36:53.924716Z"},"id":"rvAW7ymK_GdT","outputId":"eb2b4e78-159d-4cc0-9764-5326b149ce9d","trusted":true},"outputs":[],"source":["new_mask.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:36:54.323088Z","iopub.status.busy":"2023-11-15T06:36:54.322740Z","iopub.status.idle":"2023-11-15T06:36:54.328851Z","shell.execute_reply":"2023-11-15T06:36:54.328063Z","shell.execute_reply.started":"2023-11-15T06:36:54.323060Z"},"id":"DHXtvDcEw7Wc","trusted":true},"outputs":[],"source":["idx2rgb={idx:np.array(rgb) for idx, (cl, rgb) in enumerate(cls2rgb.items())}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-15T03:21:11.628828Z","iopub.status.busy":"2023-10-15T03:21:11.628587Z","iopub.status.idle":"2023-10-15T03:21:11.632486Z","shell.execute_reply":"2023-10-15T03:21:11.631455Z","shell.execute_reply.started":"2023-10-15T03:21:11.628803Z"},"trusted":true},"outputs":[],"source":["# idx2rgb"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:36:57.378211Z","iopub.status.busy":"2023-11-15T06:36:57.377888Z","iopub.status.idle":"2023-11-15T06:37:00.200045Z","shell.execute_reply":"2023-11-15T06:37:00.199268Z","shell.execute_reply.started":"2023-11-15T06:36:57.378183Z"},"id":"JcWc882tw7Wg","trusted":true},"outputs":[],"source":["# Map the idx back to rgb\n","\n","def map_class_to_rgb(p):\n","  \n","  return idx2rgb[p[0]]\n","\n","rgb_mask = np.apply_along_axis(map_class_to_rgb, -1, np.expand_dims(np.argmax(new_mask, axis=-1), -1))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T04:19:16.975140Z","iopub.status.busy":"2023-11-01T04:19:16.974798Z","iopub.status.idle":"2023-11-01T04:19:17.183484Z","shell.execute_reply":"2023-11-01T04:19:17.182491Z","shell.execute_reply.started":"2023-11-01T04:19:16.975112Z"},"id":"-XwnT2wjw7Wj","outputId":"f91a869d-7c4c-4941-f362-ae85916cbd31","trusted":true},"outputs":[],"source":["plt.imshow(rgb_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-10-14T10:52:31.510269Z","iopub.status.busy":"2023-10-14T10:52:31.509968Z","iopub.status.idle":"2023-10-14T10:52:34.077870Z","shell.execute_reply":"2023-10-14T10:52:34.076794Z","shell.execute_reply.started":"2023-10-14T10:52:31.510240Z"},"id":"jg46Yd6K73bl","jupyter":{"outputs_hidden":true,"source_hidden":true},"outputId":"ced621dd-955b-4998-fab5-5fa03970c3d3","trusted":true},"outputs":[],"source":["\n","model = unet(n_classes)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:37:00.340989Z","iopub.status.busy":"2023-11-15T06:37:00.340653Z","iopub.status.idle":"2023-11-15T06:37:00.349783Z","shell.execute_reply":"2023-11-15T06:37:00.349006Z","shell.execute_reply.started":"2023-11-15T06:37:00.340961Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import os\n","import tensorflow as tf\n","#import skimage.io as io\n","#import skimage.transform as trans\n","import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import *\n","from tensorflow.keras.callbacks import *\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.utils import *\n","from tensorflow.keras.regularizers import * \n","from keras.models import Model\n","from keras.layers import Input\n","from keras.preprocessing.image import load_img\n","from tensorflow.keras  import backend as keras"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-13T19:38:05.766201Z","iopub.status.busy":"2023-10-13T19:38:05.765889Z","iopub.status.idle":"2023-10-13T19:38:05.770802Z","shell.execute_reply":"2023-10-13T19:38:05.769487Z","shell.execute_reply.started":"2023-10-13T19:38:05.766169Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","metadata":{},"source":["# Res_PSP_V1.3 with Augmentation"]},{"cell_type":"markdown","metadata":{},"source":["## Residual_block"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:37:27.035052Z","iopub.status.busy":"2023-11-15T06:37:27.034692Z","iopub.status.idle":"2023-11-15T06:37:27.073087Z","shell.execute_reply":"2023-11-15T06:37:27.072054Z","shell.execute_reply.started":"2023-11-15T06:37:27.035017Z"},"trusted":true},"outputs":[],"source":["def res_block(in_tensor, n_filter, kernel_init, do_norm=True):\n","\n","    # in_tensor --> Incoming input\n","    # n_filter  --> Number of filters\n","    # s_kernel  --> Kernel size\n","    # do_norm   --> Use normalization or not\n","\n","    in_tensor_0 = Conv2D(n_filter , kernel_size=(1,1), padding='same', activation='relu', kernel_initializer=kernel_init)(in_tensor)\n","\n","    # ftr1 = Conv2D(n_filter , kernel_size=(7,7), padding='same', activation='relu', kernel_initializer=kernel_init)(in_tensor)\n","    ftr1 = Conv2D(n_filter , kernel_size=(3,3), dilation_rate=(5, 5), padding='same', activation='relu', kernel_initializer=kernel_init)(in_tensor)\n","    ftr1 = Conv2D(n_filter , kernel_size=(3,3), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr1)\n","    ftr1 = DepthwiseConv2D(kernel_size=(1,1), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr1)\n","    ftr1 = DepthwiseConv2D(kernel_size=(3,3), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr1)\n","    ftr1 = DepthwiseConv2D(kernel_size=(5,5), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr1)\n","    ftr1 = add([in_tensor_0, ftr1])\n","    ftr1 = BatchNormalization()(ftr1)\n","    drop1 = Dropout(0.2)(ftr1)\n","    pool1 = MaxPooling2D(2)(drop1)\n","\n","    # ftr2_res = Conv2D(n_filter * 2 , kernel_size=(5,5), padding='same', activation='relu', kernel_initializer=kernel_init)(pool1)\n","    ftr2_res = Conv2D(n_filter * 2 , kernel_size=(3, 3), dilation_rate=(3, 3), padding='same', activation='relu', kernel_initializer=kernel_init)(pool1)\n","    ftr2 = Conv2D(n_filter * 2 , kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr2_res)\n","    ftr2 = DepthwiseConv2D(kernel_size=(1,1), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr2)\n","    ftr2 = DepthwiseConv2D(kernel_size=(3,3), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr2)\n","    ftr2 = DepthwiseConv2D(kernel_size=(5,5), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr2)\n","    ftr2 = add([ftr2_res, ftr2])\n","    ftr2 = BatchNormalization()(ftr2)\n","    drop2 = Dropout(0.3)(ftr2)\n","    pool2 = MaxPooling2D(2)(drop2)\n","\n","    # ftr3_res = Conv2D(n_filter * 4 , kernel_size=(3,3), padding='same', activation='relu', kernel_initializer=kernel_init)(pool2)\n","    ftr3_res = Conv2D(n_filter * 4 , kernel_size=(3, 3), dilation_rate=(2, 2), padding='same', activation='relu', kernel_initializer=kernel_init)(pool2)\n","    ftr3 = Conv2D(n_filter * 4 , kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr3_res)\n","    ftr3 = DepthwiseConv2D(kernel_size=(1,1), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr3)\n","    ftr3 = DepthwiseConv2D(kernel_size=(3,3), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr3)\n","    ftr3 = DepthwiseConv2D(kernel_size=(5,5), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr3)\n","    ftr3 = add([ftr3_res, ftr3])\n","    ftr3 = BatchNormalization()(ftr3)\n","    drop3 = Dropout(0.3)(ftr3)\n","    pool3 = MaxPooling2D(2)(drop3)\n","\n","    up1 = UpSampling2D()(drop3)\n","    merge1 = concatenate([drop2, up1])\n","\n","    merge1 = BatchNormalization()(merge1)\n","\n","    # ftr4_res = Conv2D(n_filter * 4 , kernel_size=(3,3), padding='same', activation='relu', kernel_initializer=kernel_init)(merge1)\n","    ftr4_res = Conv2D(n_filter * 4 , kernel_size=(3, 3), dilation_rate=(2, 2), padding='same', activation='relu', kernel_initializer=kernel_init)(merge1)\n","    ftr4 = Conv2D(n_filter * 4 , kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr4_res)\n","    ftr4 = DepthwiseConv2D(kernel_size=(1,1), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr4)\n","    ftr4 = DepthwiseConv2D(kernel_size=(3,3), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr4)\n","    ftr4 = DepthwiseConv2D(kernel_size=(5,5), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr4)\n","    ftr4 = add([ftr4_res, ftr4]) # 128, 128, 256\n","    ftr4 = BatchNormalization()(ftr4)\n","    drop4 = Dropout(0.3)(ftr4)\n","\n","    up2 = UpSampling2D()(drop4) # 256, 256, 256\n","    merge2 = concatenate([drop1, up2])\n","\n","    merge2 = BatchNormalization()(merge2)\n","\n","    ftr5_res = Conv2D(n_filter, kernel_size=(3,3), padding='same', activation='relu', kernel_initializer=kernel_init)(merge2)\n","    ftr5 = Conv2D(n_filter, kernel_size=(3,3), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr5_res)\n","    ftr5 = DepthwiseConv2D(kernel_size=(1,1), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr5)\n","    ftr5 = DepthwiseConv2D(kernel_size=(3,3), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr5)\n","    ftr5 = DepthwiseConv2D(kernel_size=(5,5), padding='same', activation='relu', kernel_initializer=kernel_init)(ftr5)\n","    ftr5 = add([ftr5_res, ftr5]) # 128, 128, 256\n","    ftr5 = BatchNormalization()(ftr5)\n","    drop5 = Dropout(0.3)(ftr5)\n","\n","    if do_norm:\n","        skip_conn = add([drop5, in_tensor_0])\n","        res_bn = BatchNormalization()(skip_conn)\n","        # ups = UpSampling2D()(pool1)\n","\n","    return res_bn\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:37:51.136444Z","iopub.status.busy":"2023-11-15T06:37:51.135964Z","iopub.status.idle":"2023-11-15T06:37:51.811173Z","shell.execute_reply":"2023-11-15T06:37:51.810267Z","shell.execute_reply.started":"2023-11-15T06:37:51.136400Z"},"trusted":true},"outputs":[],"source":["def base_feature_maps(input_layer):\n","    # base covolution module to get input image feature maps\n","\n","    # # block_1\n","    # base = conv_block(input_layer,[32,32,64],'1')\n","    # # block_2\n","    # base = conv_block(base,[64,64,128],'2')\n","    # # block_3\n","    # base = conv_block(base,[128,128,256],'3')\n","    # return base\n","\n","    # base = res_block(input_layer, n_filter=32, s_kernel=3)\n","    base = res_block(input_layer, n_filter=64, s_kernel=3)\n","    base = res_block(base, n_filter=128, s_kernel=5)\n","    base = res_block(base, n_filter=256, s_kernel=7)\n","\n","    return base\n","\n","def pyramid_feature_maps(input_layer):\n","    # pyramid pooling module\n","\n","    # base = base_feature_maps(input_layer)\n","    base = res_block(input_layer, n_filter=64, kernel_init=\"he_normal\")\n","    print(base.shape)\n","    # red\n","    red = GlobalAveragePooling2D(name='red_pool')(base)\n","    red = tf.keras.layers.Reshape((1,1,-1))(red)\n","    red = Conv2D(filters=64,kernel_size=(1,1),name='red_1_by_1', activation='relu', kernel_initializer=\"he_normal\")(red)\n","#     red = BatchNormalization()(red)\n","    red = DepthwiseConv2D(kernel_size=(3,3), padding='same', activation='relu', kernel_initializer=\"he_normal\")(red)\n","    red = DepthwiseConv2D(kernel_size=(5,5), padding='same', activation='relu', kernel_initializer=\"he_normal\")(red)\n","#     red = BatchNormalization()(red)\n","    red = UpSampling2D(size=256,interpolation='bilinear',name='red_upsampling')(red)\n","\n","#     red = BatchNormalization()(red)\n","    # red = Conv2D(filters=64,kernel_size=(3,3),name='red_1_by_1', kernel_initializer=\"he_normal\")(red)\n","\n","    # yellow\n","    yellow = AveragePooling2D(pool_size=(2,2),name='yellow_pool')(base)\n","    yellow = Conv2D(filters=128,kernel_size=(1,1),name='yellow_1_by_1', activation='relu', kernel_initializer=\"he_normal\")(yellow)\n","#     yellow = BatchNormalization()(yellow)\n","    yellow = DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer=\"he_normal\")(yellow)\n","    yellow = DepthwiseConv2D(kernel_size=(5, 5), padding='same', activation='relu', kernel_initializer=\"he_normal\")(yellow)\n","#     yellow = BatchNormalization()(yellow)\n","    yellow = UpSampling2D(size=2,interpolation='bilinear',name='yellow_upsampling')(yellow)\n","\n","    # blue\n","    blue = AveragePooling2D(pool_size=(4,4),name='blue_pool')(base)\n","    blue = Conv2D(filters=256,kernel_size=(1,1),name='blue_1_by_1', activation='relu', kernel_initializer=\"he_normal\")(blue)\n","#     blue = BatchNormalization()(blue)\n","    blue = DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu',  kernel_initializer=\"he_normal\")(blue)\n","    blue = DepthwiseConv2D(kernel_size=(5, 5), padding='same', activation='relu', kernel_initializer=\"he_normal\")(blue)\n","#     blue = BatchNormalization()(blue)\n","    blue = UpSampling2D(size=4,interpolation='bilinear',name='blue_upsampling')(blue)\n","\n","    # green\n","    green = AveragePooling2D(pool_size=(8,8),name='green_pool')(base)\n","    green = Convolution2D(filters=128,kernel_size=(1,1),name='green_1_by_1', activation='relu', kernel_initializer=\"he_normal\")(green)\n","#     green = BatchNormalization()(green)\n","    green = DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer=\"he_normal\")(green)\n","    green = DepthwiseConv2D(kernel_size=(5, 5), padding='same', activation='relu', kernel_initializer=\"he_normal\")(green)\n","    green = UpSampling2D(size=8,interpolation='bilinear',name='green_upsampling')(green)\n","\n","    # base + red + yellow + blue + green\n","\n","    return BatchNormalization()(concatenate([base,red,yellow,blue,green]))\n","\n","def last_conv_module(input_layer, num_classes):\n","    X = pyramid_feature_maps(input_layer)\n","    X = Convolution2D(filters=64, kernel_size=3,padding='same', activation='relu', name='second_last_conv_3_by_3')(X)\n","    X = Convolution2D(filters=num_classes, kernel_size=3,padding='same',name='last_conv_3_by_3')(X)\n","    X = BatchNormalization(name='last_conv_3_by_3_batch_norm')(X)\n","    X = Activation('softmax' ,name='last_conv_softmax')(X)\n","    # X = Flatten(name='last_conv_flatten')(X)\n","    return X\n","\n","input_size = (256, 256, 3)\n","n_classes = 32\n","input_layer = tf.keras.Input(shape=input_size, name='input')\n","output_layer = last_conv_module(input_layer, n_classes)\n","# model_3 = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n","# model_4 = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n","# model_5 = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n","model_6 = tf.keras.Model(inputs=input_layer, outputs=output_layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-10-15T03:45:13.505906Z","iopub.status.busy":"2023-10-15T03:45:13.505533Z","iopub.status.idle":"2023-10-15T03:45:13.538483Z","shell.execute_reply":"2023-10-15T03:45:13.537472Z","shell.execute_reply.started":"2023-10-15T03:45:13.505873Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["model_3.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:04:49.947733Z","iopub.status.busy":"2023-11-01T09:04:49.947380Z","iopub.status.idle":"2023-11-01T09:04:49.973982Z","shell.execute_reply":"2023-11-01T09:04:49.973084Z","shell.execute_reply.started":"2023-11-01T09:04:49.947699Z"},"trusted":true},"outputs":[],"source":["model_4.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-10-15T09:42:08.604153Z","iopub.status.busy":"2023-10-15T09:42:08.603835Z","iopub.status.idle":"2023-10-15T09:42:08.634405Z","shell.execute_reply":"2023-10-15T09:42:08.633718Z","shell.execute_reply.started":"2023-10-15T09:42:08.604125Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["model_5.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-11-15T06:38:03.256874Z","iopub.status.busy":"2023-11-15T06:38:03.256500Z","iopub.status.idle":"2023-11-15T06:38:03.282725Z","shell.execute_reply":"2023-11-15T06:38:03.281930Z","shell.execute_reply.started":"2023-11-15T06:38:03.256842Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["model_6.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## Loading The data to memory"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:38:16.104003Z","iopub.status.busy":"2023-11-15T06:38:16.103510Z","iopub.status.idle":"2023-11-15T06:38:16.116417Z","shell.execute_reply":"2023-11-15T06:38:16.115502Z","shell.execute_reply.started":"2023-11-15T06:38:16.103945Z"},"trusted":true},"outputs":[],"source":["def load_raw_CAMVID(data_type='train', enc='ohe', shape='normal'):\n","    \n","    img_path = str(data_path) + '/' + data_type + '/'\n","    labels_path = str(data_path) + '/' + data_type + '_labels/'\n","    \n","    # without adding target_size=(256,256) in load_img we get Out of mem: 421x960x720x32x4bytes is around 34GB!\n","    x = np.array([np.array(load_img(str(img_path) + file, target_size=(256,256)))*1./255 for file in sorted(os.listdir(img_path))])\n","    \n","    if(enc=='ohe'):\n","        y = np.array([np.array(load_img(str(labels_path) + file, target_size=(256,256))) for file in sorted(os.listdir(labels_path))])\n","        \n","    elif(enc=='sparse_cat'):\n","        y = np.array([np.array(load_img(str(labels_path) + file, target_size=(256,256))) for file in sorted(os.listdir(labels_path))])\n","        \n","    if(shape == 'flat'):\n","        y = np.reshape(y.shape[0], y.shape[1]*y.shape[2])\n","        y = np.expand_dims(y, axis=-1)\n","        \n","    return x, y"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:38:23.780947Z","iopub.status.busy":"2023-11-15T06:38:23.780574Z","iopub.status.idle":"2023-11-15T06:38:52.779956Z","shell.execute_reply":"2023-11-15T06:38:52.779064Z","shell.execute_reply.started":"2023-11-15T06:38:23.780910Z"},"trusted":true},"outputs":[],"source":["import time\n","start = time.time()\n","x_train, y_train = load_raw_CAMVID(data_type='train')\n","#x_test, y_test = load_raw_CAMVID(data_type='test')# Don't load test for RAM consumption\n","x_val, y_val = load_raw_CAMVID(data_type='val')\n","end = time.time()\n","print('Time elapsed: ', end-start)\n","\n","print(x_train.shape)\n","print(y_train.shape)\n","#print(x_test.shape)\n","#print(y_test.shape)\n","print(x_val.shape)\n","print(y_val.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T07:51:46.622418Z","iopub.status.busy":"2023-11-15T07:51:46.622002Z","iopub.status.idle":"2023-11-15T07:52:01.803667Z","shell.execute_reply":"2023-11-15T07:52:01.802773Z","shell.execute_reply.started":"2023-11-15T07:51:46.622382Z"},"trusted":true},"outputs":[],"source":["x_test, y_test = load_raw_CAMVID(data_type='test')"]},{"cell_type":"markdown","metadata":{},"source":["## Augmentaton Block"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:38:52.782532Z","iopub.status.busy":"2023-11-15T06:38:52.782146Z","iopub.status.idle":"2023-11-15T06:38:53.031762Z","shell.execute_reply":"2023-11-15T06:38:53.031042Z","shell.execute_reply.started":"2023-11-15T06:38:52.782489Z"},"trusted":true},"outputs":[],"source":["# Data generator\n","batch_sz = 4\n","#https://keras.io/preprocessing/image/\n","from keras.preprocessing.image import ImageDataGenerator\n","# we create two instances with the same arguments\n","\n","# VI Note: use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n","data_gen_args = dict(rotation_range=0.2,\n","                    width_shift_range=0.05,\n","                    height_shift_range=0.05,\n","                    shear_range=0.05,\n","                    zoom_range=0.05,\n","                    horizontal_flip=True,\n","                    fill_mode='nearest')\n","                    #rescale=1./255)# Data is already scaled when loaded\n","\n","mask_gen_args = dict(rotation_range=0.2,\n","                    width_shift_range=0.05,\n","                    height_shift_range=0.05,\n","                    shear_range=0.05,\n","                    zoom_range=0.05,\n","                    horizontal_flip=True,\n","                    fill_mode='nearest')\n","                    #preprocessing_function=adjust_mask)# This is not possible since the preprocessing_function can only return the same shape as image\n","\n","image_datagen = ImageDataGenerator(**data_gen_args)\n","mask_datagen  = ImageDataGenerator(**mask_gen_args) \n","\n","# Provide the same seed and keyword arguments to the fit and flow methods\n","seed = 1\n","#image_datagen.fit(images, augment=True, seed=seed)\n","#mask_datagen.fit(masks, augment=True, seed=seed)\n","\n","image_generator = image_datagen.flow(\n","    x_train,\n","    seed=seed,\n","    batch_size=batch_sz)\n","\n","mask_generator = mask_datagen.flow( \n","    y_train,\n","    seed=seed,\n","    batch_size=batch_sz)\n","\n","# combine generators into one which yields image and masks\n","train_generator = zip(image_generator, mask_generator)\n","\n","def train_generator_fn():\n","\n","    for (img, mask) in train_generator:\n","        new_mask = adjust_mask(mask)\n","        yield (img, new_mask)  \n","        \n","val_image_generator = image_datagen.flow(\n","    x_val,\n","    seed=seed,\n","    batch_size=batch_sz)\n","\n","val_mask_generator = mask_datagen.flow(\n","    y_val,\n","    seed=seed,\n","    batch_size=batch_sz)\n","\n","# combine generators into one which yields image and masks\n","val_generator = zip(val_image_generator, val_mask_generator)        \n","        \n","def val_generator_fn():\n","\n","    for (img, mask) in val_generator:\n","        new_mask = adjust_mask(mask)\n","        yield (img, new_mask)         \n"]},{"cell_type":"markdown","metadata":{},"source":["## Metrics and losses"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:39:00.194485Z","iopub.status.busy":"2023-11-15T06:39:00.194133Z","iopub.status.idle":"2023-11-15T06:39:00.216948Z","shell.execute_reply":"2023-11-15T06:39:00.216097Z","shell.execute_reply.started":"2023-11-15T06:39:00.194453Z"},"trusted":true},"outputs":[],"source":["def train_generator_fn():\n","    for (img,mask) in train_generator:\n","    \n","        \n","        new_mask = adjust_mask(mask)\n","        yield (img,new_mask)   \n","        \n","def val_generator_fn():\n","    for (img,mask) in val_generator:\n","        new_mask = adjust_mask(mask)\n","        yield (img,new_mask)  \n","\n","        \n","## Metrics\n","\n","def dice(y_true, y_pred, smooth=1):\n","    \n","    intersection = K.sum(y_true * y_pred, axis=[-1])\n","    union = K.sum(y_true, axis=[-1]) + K.sum(y_pred, axis=[-1])\n","    dicef = K.mean((2. * intersection + smooth)/(union + smooth), axis=-1)\n","    return dicef\n","\n","\n","def IOU(y_true, y_pred, smooth=1):\n","\n","    intersection = K.sum(y_true * y_pred, axis=[-1])\n","    union = K.sum(y_true, axis=[-1]) + K.sum(y_pred, axis=[-1])-intersection\n","    iou_scr = K.mean(( intersection + smooth)/(union + smooth), axis=-1)\n","    return iou_scr\n","\n","def IOU_loss(y_true, y_pred, smooth=1):\n","\n","    intersection = K.sum(y_true * y_pred, axis=[-1])\n","    union = K.sum(y_true, axis=[-1]) + K.sum(y_pred, axis=[-1])-intersection\n","    iou_scr = K.mean(( intersection + smooth)/(union + smooth), axis=-1)\n","    return 1 - iou_scr\n","\n","\n","alpha_vals = np.ones((1, n_classes))  * 0.25\n","\n","def categorical_focal_loss(alpha, gamma=2.):\n","    \"\"\"\n","    Softmax version of focal loss.\n","    When there is a skew between different categories/labels in your data set, you can try to apply this function as a\n","    loss.\n","           m\n","      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n","          c=1\n","\n","      where m = number of classes, c = class and o = observation\n","\n","    Parameters:\n","      alpha -- the same as weighing factor in balanced cross entropy. Alpha is used to specify the weight of different\n","      categories/labels, the size of the array needs to be consistent with the number of classes.\n","      gamma -- focusing parameter for modulating factor (1-p)\n","\n","    Default value:\n","      gamma -- 2.0 as mentioned in the paper\n","      alpha -- 0.25 as mentioned in the paper\n","\n","    References:\n","        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n","        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n","\n","    Usage:\n","     model.compile(loss=[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n","    \"\"\"\n","\n","    alpha = np.array(alpha, dtype=np.float32)\n","\n","    def categorical_focal_loss_fixed(y_true, y_pred):\n","        \"\"\"\n","        :param y_true: A tensor of the same shape as `y_pred`\n","        :param y_pred: A tensor resulting from a softmax\n","        :return: Output tensor.\n","        \"\"\"\n","\n","        # Clip the prediction value to prevent NaN's and Inf's\n","        epsilon = K.epsilon()\n","        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n","\n","        # Calculate Cross Entropy\n","        cross_entropy = -y_true * K.log(y_pred)\n","\n","        # Calculate Focal Loss\n","        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n","\n","        # Compute mean loss in mini_batch\n","        return K.mean(K.sum(loss, axis=-1))\n","\n","    return categorical_focal_loss_fixed"]},{"cell_type":"markdown","metadata":{},"source":["## Helper Callbacks"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:39:33.109280Z","iopub.status.busy":"2023-11-15T06:39:33.108897Z","iopub.status.idle":"2023-11-15T06:39:33.120233Z","shell.execute_reply":"2023-11-15T06:39:33.119172Z","shell.execute_reply.started":"2023-11-15T06:39:33.109248Z"},"trusted":true},"outputs":[],"source":["import keras.callbacks as callbacks\n","from keras.callbacks import Callback\n","\n","class SnapshotCallbackBuilder:\n","    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n","        self.T = nb_epochs\n","        self.M = nb_snapshots\n","        self.alpha_zero = init_lr\n","\n","    def get_callbacks(self, model_prefix='Model'):\n","\n","        callback_list = [\n","            callbacks.ModelCheckpoint(\"./pspnet_camvid_1.hdf5\",monitor='val_loss', \n","                                   mode = 'min', save_best_only=True, verbose=1),\n","#             swa,\n","            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule),\n","            callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, mode='min', verbose=1, factor=0.5) # used on model_4\n","#             callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, mode='min', verbose=1, factor=0.2) # used on model_5\n","        ]\n","\n","        return callback_list\n","\n","    def _cosine_anneal_schedule(self, t):\n","        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n","        cos_inner /= self.T // self.M\n","        cos_out = np.cos(cos_inner) + 1\n","        return float(self.alpha_zero / 2 * cos_out)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:41:32.802990Z","iopub.status.busy":"2023-11-15T06:41:32.802657Z","iopub.status.idle":"2023-11-15T06:41:32.820034Z","shell.execute_reply":"2023-11-15T06:41:32.819062Z","shell.execute_reply.started":"2023-11-15T06:41:32.802961Z"},"trusted":true},"outputs":[],"source":["# Model 3 #\n","\n","Adam =  tf.keras.optimizers.Adam\n","# model.compile(optimizer = Adam(lr = 1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy',dice, IOU])\n","\n","# model_2.compile(optimizer = Adam(lr = 1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy', dice, IOU])\n","\n","# model_3.compile(optimizer='adam', loss=[categorical_focal_loss(alpha=alpha_vals, gamma=2.)], \n","#                 metrics = ['accuracy', dice, IOU])\n","\n","# model_4.compile(optimizer='adam', loss=[categorical_focal_loss(alpha=alpha_vals, gamma=2.)], \n","#                 metrics = ['accuracy', dice, IOU])\n","\n","# model_5.compile(optimizer='adam', loss=[categorical_focal_loss(alpha=alpha_vals, gamma=2.)], \n","#                 metrics = ['accuracy', dice, IOU])\n","\n","\n","model_6.compile(optimizer='adam', loss = 'categorical_crossentropy', \n","                 metrics = ['accuracy', dice, IOU])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T06:41:50.497108Z","iopub.status.busy":"2023-11-15T06:41:50.496741Z","iopub.status.idle":"2023-11-15T07:45:30.491491Z","shell.execute_reply":"2023-11-15T07:45:30.490538Z","shell.execute_reply.started":"2023-11-15T06:41:50.497072Z"},"trusted":true},"outputs":[],"source":["BASE_PATH = \"/kaggle/input/camvid/CamVid\"\n","view = 0\n","batch_sz = 4\n","epochs = 50\n","validation_steps = 32\n","\n","n_train_samples = len(os.listdir(str(BASE_PATH) + '/train/'))\n","snapshot = SnapshotCallbackBuilder(nb_epochs=epochs, nb_snapshots=1, init_lr=1e-3)\n","# n_train_samples\n","\n","# model_checkpoint = ModelCheckpoint('pspnet_camvid.hdf5', monitor='val_loss', verbose=1, save_best_only=True)\n","\n","\n","model_6.fit_generator(train_generator_fn(),\n","                    validation_data=val_generator_fn(),\n","                    steps_per_epoch=n_train_samples//batch_sz,\n","                    validation_steps=validation_steps,\n","                    epochs=epochs,\n","#                     callbacks=[model_checkpoint]\n","                    callbacks=snapshot.get_callbacks())"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizing model performance"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T07:46:57.096019Z","iopub.status.busy":"2023-11-15T07:46:57.095634Z","iopub.status.idle":"2023-11-15T07:46:57.108297Z","shell.execute_reply":"2023-11-15T07:46:57.107357Z","shell.execute_reply.started":"2023-11-15T07:46:57.095985Z"},"trusted":true},"outputs":[],"source":["def visualize_seg(img, gt_mask, get_model=None, shape='normal', gt_mode='sparse'):\n","    fig , ax = plt.subplots(1,3,figsize=(15,15))\n","  \n","  # Img\n","    ax[0].imshow(img)\n","    ax[0].set_title(\"Orignal Image\")\n","  \n","  # Predict\n","    pred_mask = get_model.predict(np.expand_dims(img, 0))\n","    pred_mask = np.argmax(pred_mask, axis=-1)\n","    pred_mask = pred_mask[0]\n","    if shape=='flat':\n","        pred_mask = np.reshape(pred_mask, (256,256)) # Reshape only if you use the flat model. O.w. you dont need\n","  \n","    rgb_mask = np.apply_along_axis(map_class_to_rgb, -1, np.expand_dims(pred_mask, -1))\n","  \n","  # Prediction\n","    ax[1].imshow(rgb_mask)\n","    ax[1].set_title(\"Predicted Mask\")\n","\n","              \n","  # GT mask\n","    if gt_mode == 'ohe':\n","        gt_img_ohe = np.argmax(gt_mask, axis=-1)\n","        gt_mask = np.apply_along_axis(map_class_to_rgb, -1, np.expand_dims(gt_img_ohe, -1))              \n","  \n","    ax[2].imshow((gt_mask).astype(np.uint8))\n","    ax[2].set_title(\"Ground truth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T07:47:16.588126Z","iopub.status.busy":"2023-11-15T07:47:16.587784Z","iopub.status.idle":"2023-11-15T07:47:17.961636Z","shell.execute_reply":"2023-11-15T07:47:17.960765Z","shell.execute_reply.started":"2023-11-15T07:47:16.588095Z"},"trusted":true},"outputs":[],"source":["img = next(val_image_generator)[0]\n","gt_img = next(val_mask_generator)[0]\n","# visualize_seg(img, gt_img, get_model=model_4, gt_mode='sparse')\n","visualize_seg(img, gt_img, get_model=model_6, gt_mode='sparse')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T07:54:02.809495Z","iopub.status.busy":"2023-11-15T07:54:02.809129Z","iopub.status.idle":"2023-11-15T07:54:03.569400Z","shell.execute_reply":"2023-11-15T07:54:03.568573Z","shell.execute_reply.started":"2023-11-15T07:54:02.809459Z"},"trusted":true},"outputs":[],"source":["img = x_val[1]\n","gt_img = y_val[1]\n","# visualize_seg(img, gt_img, get_model=model_4, gt_mode='sparse')\n","visualize_seg(img, gt_img, get_model=model_6, gt_mode='sparse')"]},{"cell_type":"markdown","metadata":{},"source":["## Computing class-wise perforamce"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T08:02:25.654143Z","iopub.status.busy":"2023-11-15T08:02:25.653801Z","iopub.status.idle":"2023-11-15T08:02:25.662646Z","shell.execute_reply":"2023-11-15T08:02:25.661763Z","shell.execute_reply.started":"2023-11-15T08:02:25.654112Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(y_true, y_pred):\n","    '''\n","    Computes IOU and Dice Score.\n","\n","    Args:\n","    y_true (tensor) - ground truth label map\n","    y_pred (tensor) - predicted label map\n","    '''\n","\n","    class_wise_iou = []\n","    class_wise_dice_score = []\n","\n","    smoothening_factor = 0.00001\n","\n","    for i in range(32):\n","        intersection = np.sum((y_pred == i) * (y_true == i))\n","        y_true_area = np.sum((y_true == i))\n","        y_pred_area = np.sum((y_pred == i))\n","        combined_area = y_true_area + y_pred_area\n","\n","        iou = (intersection + smoothening_factor) / (combined_area - intersection + smoothening_factor)\n","        class_wise_iou.append(iou)\n","\n","        dice_score =  2 * ((intersection + smoothening_factor) / (combined_area + smoothening_factor))\n","        class_wise_dice_score.append(dice_score)\n","\n","    return class_wise_iou, class_wise_dice_score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T08:03:08.722768Z","iopub.status.busy":"2023-11-15T08:03:08.722418Z","iopub.status.idle":"2023-11-15T08:03:09.088454Z","shell.execute_reply":"2023-11-15T08:03:09.087263Z","shell.execute_reply.started":"2023-11-15T08:03:08.722737Z"},"trusted":true},"outputs":[],"source":["img = x_test[1]\n","gt_img = y_test[1]\n","model_name = model_6\n","p_img = model_name.predict(np.expand_dims(img, 0))[0]\n","pred_mask = np.argmax(p_img, axis=-1)\n","rgb_mask = np.apply_along_axis(map_class_to_rgb, -1, np.expand_dims(pred_mask, -1))\n","im_iou, im_dice = compute_metrics(np.uint8(gt_img),np.uint8(rgb_mask))\n","\n","print('iou score, dice score', im_iou, im_dice)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T07:59:49.390181Z","iopub.status.busy":"2023-11-15T07:59:49.389808Z","iopub.status.idle":"2023-11-15T07:59:49.397689Z","shell.execute_reply":"2023-11-15T07:59:49.396664Z","shell.execute_reply.started":"2023-11-15T07:59:49.390151Z"},"trusted":true},"outputs":[],"source":["np.uint8(rgb_mask)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":635428,"sourceId":1132317,"sourceType":"datasetVersion"}],"dockerImageVersionId":30043,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
